# -*- coding: utf-8 -*-
"""Fabric-MobileNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OHrTuhVNfLITkPpYLpb0rGRDly_SEc-q
"""

pip install bayesian-optimization

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.applications import imagenet_utils
from sklearn.metrics import confusion_matrix
from bayes_opt import BayesianOptimization
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import itertools
import os
import shutil
import random
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(physical_devices))

train_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/train'
valid_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/valid'
test_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/test'

len(os.listdir('/content/drive/MyDrive/Thesis/Fabric Defect Dataset/test/hole'))

"""# Pre-processing and model training



"""

train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=train_path, target_size=(224,224), batch_size=10, class_mode="sparse")
valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=valid_path, target_size=(224,224), batch_size=10, class_mode="sparse")
test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False, class_mode="sparse")

test_batches.class_indices

mobile = tf.keras.applications.mobilenet.MobileNet()
mobile.summary()

x = mobile.layers[-5].output
output = Dense(units=3, activation='softmax')(x)

model = Model(inputs=mobile.input, outputs=output)
len(model.layers)

for layer in model.layers[:-22]:
    layer.trainable = False

model.summary()

model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

plot_model(model, to_file='mobilenet_architecture.png')

model_result = model.fit(x=train_batches,
            validation_data=test_batches,
            epochs=50,
            verbose=2
)

max_accuracy = max(model_result.history['val_accuracy'])
epoch = model_result.history['val_accuracy'].index(max_accuracy)
print('Max Accuracy: ',max_accuracy)
print('Epoch: ', epoch)

plt.figure(figsize=(8, 6))
for c in ['loss', 'val_loss']:
    plt.plot(model_result.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Negative Log Likelihood')
plt.title('Training and Validation Losses')

plt.figure(figsize=(8, 6))
for c in ['accuracy', 'val_accuracy']:
    plt.plot(model_result.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Accuracy')
plt.title('Training and Validation Accuracy')

"""# **Precision, Recall, and F1 Score**"""

y_true = test_batches.classes
predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)
y_pred = []
for i in range(0,len(y_true)):
  y_pred.append(predictions.argmax(axis=3)[i][0][0])

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#calculating precision and reall
cm = confusion_matrix(y_true=y_true, y_pred=y_pred)
true_pos = np.diag(cm)
false_pos = np.sum(cm, axis=0) - true_pos
false_neg = np.sum(cm, axis=1) - true_pos

precision = np.average(true_pos / (true_pos + false_pos))
recall = np.average(true_pos / (true_pos + false_neg))
F1 = 2 * (precision * recall) / (precision + recall)

print('Precision: ',precision)
print('Recall: ',recall)
print('F1 Score: ',F1)

#calculating precision and reall
precision = precision_score(y_true, y_pred, average='micro')
recall = recall_score(y_true, y_pred, average='micro')
F1 = f1_score(y_true, y_pred, average='micro')

print('Precision: ',precision)
print('Recall: ',recall)
print('F1 Score: ',F1)

"""

# **Bayesian Optimizer**"""

# Define the objective function for Bayesian optimization
def objective(learning_rate, dropout_rate):
    # Update the model with the new hyperparameters
    optimizer.learning_rate = learning_rate
    model.layers[-5].rate = dropout_rate
    model.layers[-7].rate = dropout_rate

    # Train the model
    history = model.fit(x=train_batches,
            validation_data=test_batches,
            epochs=20,
            verbose=2
)

    # Return the validation accuracy for optimization
    return max(history.history['val_accuracy'])

# Define the parameter ranges for Bayesian optimization
pbounds = {'learning_rate': (1e-6, 1e-2),
           'dropout_rate': (0.0, 0.5)}

# Run Bayesian optimization
optimizer = BayesianOptimization(f=objective, pbounds=pbounds, verbose=2)
optimizer.maximize(init_points=2, n_iter=2)

# Get the best hyperparameters
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_dropout_rate = best_params['dropout_rate']

# Update the model with the best hyperparameters
optimizer.learning_rate = best_learning_rate
model.layers[-5].rate = best_dropout_rate
model.layers[-7].rate = best_dropout_rate

# Train the model with the best hyperparameters
history = model.fit(x=train_batches,
            validation_data=test_batches,
            epochs=20,
            verbose=2
)

"""# Confusion matrix, precision, and recall





"""

test_labels = list(test_batches.classes)

prediction = model.predict(x=test_batches, verbose=0)

prediction_labels = []
for i in range(0,45):
  prediction_labels.append(prediction.argmax(axis=3)[i][0][0])

cm = confusion_matrix(y_true=test_labels[:45], y_pred=prediction_labels)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

cm_plot_labels = ['horizontal','vertical','hole']
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')

recall = np.diag(cm) / np.sum(cm, axis = 1)
recall = np.mean(recall)
print('Recall:', recall)

precision = np.diag(cm) / np.sum(cm, axis = 0)
precision = np.mean(precision)
print('Precision:', precision)

plt.figure(figsize=(8, 6))
for c in ['loss', 'val_loss']:
    plt.plot(model_result.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Negative Log Likelihood')
plt.title('Training and Validation Losses')

plt.figure(figsize=(8, 6))
for c in ['accuracy', 'val_accuracy']:
    plt.plot(model_result.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Accuracy')
plt.title('Training and Validation Accuracy')

"""# **Precision, Recall, and F1 Score**"""

y_true = test_batches.classes
predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)
y_pred = []
for i in range(0,len(y_true)):
  y_pred.append(predictions.argmax(axis=3)[i][0][0])

y_pred

#calculating precision and reall
cm = confusion_matrix(y_true=y_true, y_pred=y_pred)
true_pos = np.diag(cm)
false_pos = np.sum(cm, axis=0) - true_pos
false_neg = np.sum(cm, axis=1) - true_pos

precision = np.average(true_pos / (true_pos + false_pos))
recall = np.average(true_pos / (true_pos + false_neg))
F1 = 2 * (precision * recall) / (precision + recall)

print('Precision: ',precision)
print('Recall: ',recall)
print('F1 Score: ',F1)

plot_confusion_matrix(cm=cm, classes=['Hole','Horizontal','Vertical'], title='Confusion Matrix')

